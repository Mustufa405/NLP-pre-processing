{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b7aa883-5c16-4bf2-9dcd-7637460239ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cont = CountVectorizer()\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "t = TfidfVectorizer()\n",
    "from nltk.stem import PorterStemmer\n",
    "port = PorterStemmer()\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lm = WordNetLemmatizer()\n",
    "import tensorflow\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tok = Tokenizer()\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86357cbc-bac2-456d-9347-abe63ab129f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    "<h1>sooo yesterday!!!</h1> i taught the <b>students</b> about <i>nlp preprocessing</i> , yeah yeah like lower  CASE , stop words, and punctuation ... oh wow!! what a  mess mess... there were about 21 or 22 students 999 888 maybe ?? not sure ü§î anyway  we talked and talked  and talked about commas, dots, dots... more dots..... ??? question marks, semi;colons;;; and other stuff !!!!  \n",
    "\n",
    "then some <div> of </div> them said like ‚Äúsir why we remove punctuation 123 456 789 numbers , html tags <p></p> and spaces spaces   and more  spaces ???‚Äù i said because it makes <code>the data clean clean clean</code> oh yes YES yes.  \n",
    "\n",
    "we also found sooo many stopwords like \"the\" \"a\" \"an\" \"is\" \"of\" \"in\" \"to\" \"on\" \"for\" <span>and</span>  \"that\" \"by\" oh nooo, and ugh  repeated words everywhere everywhere everywhere !!!  \n",
    "\n",
    "sometimes  the  text  had  BIG  letters, small letters, Missing letters, ...  punctuation,  punctuation,  punctuation!!!  üò©üò© lowercase is important, uppercase looks  BAD BAD BAD!!!  \n",
    "\n",
    "and  we  found  multiple   spaces     like     this     one     and    even     worse    ones.   some sentences ended ??? without any reason ... others ended !!!!!!! or ........ or 1234567890 randomly inserted in middle of text üòê .  \n",
    "\n",
    "students were like <a href=\"#\">sir we tired</a> üò¥ but i said ‚Äúno!! clean the DATA!! remove html tags <img> <body> <head>, remove punctuation!!!!! remove stopwords, remove numbers 000111222333, remove emojis üòÖüòÇüêç, remove extra spaces!!! ‚Äù  \n",
    "\n",
    "then one student wrote: REMOVE   STOPWORDS  PLEASE!!!!!! PLEASE  PLEASE!!!! in ALL CAPS üò≠üò≠üò≠ and forgot half of the text lol lol lol.  \n",
    "\n",
    "we used nltk , regex , and re.sub() and .split() and join() join() join()  functions ... oh so many ( ) ( ) [ ] { } symbols; ; ; ; !!!  \n",
    "\n",
    "btw <html> we </html> also found that lowercase makes the <p>text</p> more readable readable readable readable readable !!! not LIKE THIS or LIKE THAT .  \n",
    "\n",
    "<article> after </article> finishing <section> class </section> , we all laughed, smiled, cleaned cleaned data, removed numbers(12345 67890), stopwords stopwords stopwords stopwords, and extra  spaces!!!  \n",
    "\n",
    "finally the dataset looked better better better better, before it was a big mess mess mess mess mess full of tags <div> <meta> <h1> </h1> <title> broken html <br> <hr> </hr> punctuation!!!  \n",
    "\n",
    "some text was like:  <b>this.. is.. just.. messy.. text..!!!</b>  others like  what??? why???? no idea... anyway.  \n",
    "\n",
    "cleaning text is so boring boring boring but also fun fun fun!!! it makes model smarter, faster, better, clearer, not slower slower slower üòúüòúüòú.  \n",
    "\n",
    "and i said again again again, ‚Äúdon‚Äôt forget to remove punctuation, stopwords, numbers, emojis, and html tags!!!‚Äù  \n",
    "\n",
    "then they all said ‚Äúyes sir yes sir‚Äù three times three times three times.  \n",
    "\n",
    "<footer> and </footer> that‚Äôs how preprocessing works works works ‚Äî lower, clean, trim, remove, filter, normalize normalize normalize normalize!!!'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbb7320-4e8b-4dbd-bed9-7b7cd773e925",
   "metadata": {},
   "source": [
    "### text lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "872b6fa4-c0f4-4c45-81e0-05ac1268b8aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n<h1>sooo yesterday!!!</h1> i taught the <b>students</b> about <i>nlp preprocessing</i> , yeah yeah like lower  case , stop words, and punctuation ... oh wow!! what a  mess mess... there were about 21 or 22 students 999 888 maybe ?? not sure ü§î anyway  we talked and talked  and talked about commas, dots, dots... more dots..... ??? question marks, semi;colons;;; and other stuff !!!!  \\n\\nthen some <div> of </div> them said like ‚Äúsir why we remove punctuation 123 456 789 numbers , html tags <p></p> and spaces spaces   and more  spaces ???‚Äù i said because it makes <code>the data clean clean clean</code> oh yes yes yes.  \\n\\nwe also found sooo many stopwords like \"the\" \"a\" \"an\" \"is\" \"of\" \"in\" \"to\" \"on\" \"for\" <span>and</span>  \"that\" \"by\" oh nooo, and ugh  repeated words everywhere everywhere everywhere !!!  \\n\\nsometimes  the  text  had  big  letters, small letters, missing letters, ...  punctuation,  punctuation,  punctuation!!!  üò©üò© lowercase is important, uppercase looks  bad bad bad!!!  \\n\\nand  we  found  multiple   spaces     like     this     one     and    even     worse    ones.   some sentences ended ??? without any reason ... others ended !!!!!!! or ........ or 1234567890 randomly inserted in middle of text üòê .  \\n\\nstudents were like <a href=\"#\">sir we tired</a> üò¥ but i said ‚Äúno!! clean the data!! remove html tags <img> <body> <head>, remove punctuation!!!!! remove stopwords, remove numbers 000111222333, remove emojis üòÖüòÇüêç, remove extra spaces!!! ‚Äù  \\n\\nthen one student wrote: remove   stopwords  please!!!!!! please  please!!!! in all caps üò≠üò≠üò≠ and forgot half of the text lol lol lol.  \\n\\nwe used nltk , regex , and re.sub() and .split() and join() join() join()  functions ... oh so many ( ) ( ) [ ] { } symbols; ; ; ; !!!  \\n\\nbtw <html> we </html> also found that lowercase makes the <p>text</p> more readable readable readable readable readable !!! not like this or like that .  \\n\\n<article> after </article> finishing <section> class </section> , we all laughed, smiled, cleaned cleaned data, removed numbers(12345 67890), stopwords stopwords stopwords stopwords, and extra  spaces!!!  \\n\\nfinally the dataset looked better better better better, before it was a big mess mess mess mess mess full of tags <div> <meta> <h1> </h1> <title> broken html <br> <hr> </hr> punctuation!!!  \\n\\nsome text was like:  <b>this.. is.. just.. messy.. text..!!!</b>  others like  what??? why???? no idea... anyway.  \\n\\ncleaning text is so boring boring boring but also fun fun fun!!! it makes model smarter, faster, better, clearer, not slower slower slower üòúüòúüòú.  \\n\\nand i said again again again, ‚Äúdon‚Äôt forget to remove punctuation, stopwords, numbers, emojis, and html tags!!!‚Äù  \\n\\nthen they all said ‚Äúyes sir yes sir‚Äù three times three times three times.  \\n\\n<footer> and </footer> that‚Äôs how preprocessing works works works ‚Äî lower, clean, trim, remove, filter, normalize normalize normalize normalize!!!'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = text.lower()\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faef2d31-a24b-4fc2-9f00-b8b74c53395f",
   "metadata": {},
   "source": [
    "### HTML Tags Remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b34b1f7d-e84f-440a-af8b-55cfd57531ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = re.sub(r'<.*?>', '', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fe3c0a-aad1-4aa6-97c1-1ed6efd9fbd2",
   "metadata": {},
   "source": [
    "### Number Remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "815612d4-127b-4f34-9535-fed5eeee7149",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = re.sub(\"\\d+\",\"\",text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c43e1f-5cd5-4530-9d89-fdad74b2425e",
   "metadata": {},
   "source": [
    "### Punctuation Remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47d22cdd-74fc-43f1-a9fb-cbd05f4719cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nhsooo yesterdayh i taught the bstudentsb about inlp preprocessingi  yeah yeah like lower  case  stop words and punctuation  oh wow what a  mess mess there were about  or  students   maybe  not sure  anyway  we talked and talked  and talked about commas dots dots more dots  question marks semicolons and other stuff   \\n\\nthen some div of div them said like sir why we remove punctuation    numbers  html tags pp and spaces spaces   and more  spaces  i said because it makes codethe data clean clean cleancode oh yes yes yes  \\n\\nwe also found sooo many stopwords like the a an is of in to on for spanandspan  that by oh nooo and ugh  repeated words everywhere everywhere everywhere   \\n\\nsometimes  the  text  had  big  letters small letters missing letters   punctuation  punctuation  punctuation   lowercase is important uppercase looks  bad bad bad  \\n\\nand  we  found  multiple   spaces     like     this     one     and    even     worse    ones   some sentences ended  without any reason  others ended  or  or  randomly inserted in middle of text    \\n\\nstudents were like a hrefsir we tireda  but i said no clean the data remove html tags img body head remove punctuation remove stopwords remove numbers  remove emojis  remove extra spaces   \\n\\nthen one student wrote remove   stopwords  please please  please in all caps  and forgot half of the text lol lol lol  \\n\\nwe used nltk  regex  and resub and split and join join join  functions  oh so many         symbols      \\n\\nbtw html we html also found that lowercase makes the ptextp more readable readable readable readable readable  not like this or like that   \\n\\narticle after article finishing section class section  we all laughed smiled cleaned cleaned data removed numbers  stopwords stopwords stopwords stopwords and extra  spaces  \\n\\nfinally the dataset looked better better better better before it was a big mess mess mess mess mess full of tags div meta h h title broken html br hr hr punctuation  \\n\\nsome text was like  bthis is just messy textb  others like  what why no idea anyway  \\n\\ncleaning text is so boring boring boring but also fun fun fun it makes model smarter faster better clearer not slower slower slower   \\n\\nand i said again again again dont forget to remove punctuation stopwords numbers emojis and html tags  \\n\\nthen they all said yes sir yes sir three times three times three times  \\n\\nfooter and footer thats how preprocessing works works works  lower clean trim remove filter normalize normalize normalize normalize'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = re.sub(r'[^\\w\\s]', '', text)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d53af-e217-4f89-8e77-eed309d4e4ad",
   "metadata": {},
   "source": [
    "### Stopwords Remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5053ac3-dc74-451b-ade0-69f90641383a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hsooo yesterdayh taught bstudentsb inlp preprocessingi yeah yeah like lower case stop words punctuation oh wow mess mess students maybe sure anyway talked talked talked commas dots dots dots question marks semicolons stuff div div said like sir remove punctuation numbers html tags pp spaces spaces spaces said makes codethe data clean clean cleancode oh yes yes yes also found sooo many stopwords like spanandspan oh nooo ugh repeated words everywhere everywhere everywhere sometimes text big letters small letters missing letters punctuation punctuation punctuation lowercase important uppercase looks bad bad bad found multiple spaces like one even worse ones sentences ended without reason others ended randomly inserted middle text students like hrefsir tireda said clean data remove html tags img body head remove punctuation remove stopwords remove numbers remove emojis remove extra spaces one student wrote remove stopwords please please please caps forgot half text lol lol lol used nltk regex resub split join join join functions oh many symbols btw html html also found lowercase makes ptextp readable readable readable readable readable like like article article finishing section class section laughed smiled cleaned cleaned data removed numbers stopwords stopwords stopwords stopwords extra spaces finally dataset looked better better better better big mess mess mess mess mess full tags div meta h h title broken html br hr hr punctuation text like bthis messy textb others like idea anyway cleaning text boring boring boring also fun fun fun makes model smarter faster better clearer slower slower slower said dont forget remove punctuation stopwords numbers emojis html tags said yes sir yes sir three times three times three times footer footer thats preprocessing works works works lower clean trim remove filter normalize normalize normalize normalize'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \" \".join([m for m in text.split() if m not in stopwords.words(\"english\")])\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729f119f-55c9-4d10-9ca7-80bf10a475a1",
   "metadata": {},
   "source": [
    "### Extra Space Remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b030fb4d-32c6-47e1-aacb-775337dad32c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hsooo yesterdayh taught bstudentsb inlp preprocessingi yeah yeah like lower case stop words punctuation oh wow mess mess students maybe sure anyway talked talked talked commas dots dots dots question marks semicolons stuff div div said like sir remove punctuation numbers html tags pp spaces spaces spaces said makes codethe data clean clean cleancode oh yes yes yes also found sooo many stopwords like spanandspan oh nooo ugh repeated words everywhere everywhere everywhere sometimes text big letters small letters missing letters punctuation punctuation punctuation lowercase important uppercase looks bad bad bad found multiple spaces like one even worse ones sentences ended without reason others ended randomly inserted middle text students like hrefsir tireda said clean data remove html tags img body head remove punctuation remove stopwords remove numbers remove emojis remove extra spaces one student wrote remove stopwords please please please caps forgot half text lol lol lol used nltk regex resub split join join join functions oh many symbols btw html html also found lowercase makes ptextp readable readable readable readable readable like like article article finishing section class section laughed smiled cleaned cleaned data removed numbers stopwords stopwords stopwords stopwords extra spaces finally dataset looked better better better better big mess mess mess mess mess full tags div meta h h title broken html br hr hr punctuation text like bthis messy textb others like idea anyway cleaning text boring boring boring also fun fun fun makes model smarter faster better clearer slower slower slower said dont forget remove punctuation stopwords numbers emojis html tags said yes sir yes sir three times three times three times footer footer thats preprocessing works works works lower clean trim remove filter normalize normalize normalize normalize'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \" \".join(text.split())\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13df33dc-e592-49cb-8b69-dfa420d3d5be",
   "metadata": {},
   "source": [
    "### Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d79fde02-68fd-464d-90d2-dc0b7e0749ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "co = cont.fit_transform([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38d920f3-5007-4271-b39f-7c8472c6cfac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['also', 'anyway', 'article', 'bad', 'better', 'big', 'body',\n",
       "       'boring', 'br', 'broken', 'bstudentsb', 'bthis', 'btw', 'caps',\n",
       "       'case', 'class', 'clean', 'cleancode', 'cleaned', 'cleaning',\n",
       "       'clearer', 'codethe', 'commas', 'data', 'dataset', 'div', 'dont',\n",
       "       'dots', 'emojis', 'ended', 'even', 'everywhere', 'extra', 'faster',\n",
       "       'filter', 'finally', 'finishing', 'footer', 'forget', 'forgot',\n",
       "       'found', 'full', 'fun', 'functions', 'half', 'head', 'hr',\n",
       "       'hrefsir', 'hsooo', 'html', 'idea', 'img', 'important', 'inlp',\n",
       "       'inserted', 'join', 'laughed', 'letters', 'like', 'lol', 'looked',\n",
       "       'looks', 'lower', 'lowercase', 'makes', 'many', 'marks', 'maybe',\n",
       "       'mess', 'messy', 'meta', 'middle', 'missing', 'model', 'multiple',\n",
       "       'nltk', 'nooo', 'normalize', 'numbers', 'oh', 'one', 'ones',\n",
       "       'others', 'please', 'pp', 'preprocessing', 'preprocessingi',\n",
       "       'ptextp', 'punctuation', 'question', 'randomly', 'readable',\n",
       "       'reason', 'regex', 'remove', 'removed', 'repeated', 'resub',\n",
       "       'said', 'section', 'semicolons', 'sentences', 'sir', 'slower',\n",
       "       'small', 'smarter', 'smiled', 'sometimes', 'sooo', 'spaces',\n",
       "       'spanandspan', 'split', 'stop', 'stopwords', 'student', 'students',\n",
       "       'stuff', 'sure', 'symbols', 'tags', 'talked', 'taught', 'text',\n",
       "       'textb', 'thats', 'three', 'times', 'tireda', 'title', 'trim',\n",
       "       'ugh', 'uppercase', 'used', 'without', 'words', 'works', 'worse',\n",
       "       'wow', 'wrote', 'yeah', 'yes', 'yesterdayh'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d900bfb-5ddc-48d4-a1d3-4634444537b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3,  2,  2,  3,  5,  2,  1,  3,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         4,  1,  2,  1,  1,  1,  1,  3,  1,  3,  1,  3,  2,  2,  1,  3,\n",
       "         2,  1,  1,  1,  1,  2,  1,  1,  3,  1,  3,  1,  1,  1,  2,  1,\n",
       "         1,  6,  1,  1,  1,  1,  1,  3,  1,  3,  9,  3,  1,  1,  2,  2,\n",
       "         3,  2,  1,  1,  7,  1,  1,  1,  1,  1,  1,  1,  1,  4,  4,  4,\n",
       "         2,  1,  2,  3,  1,  1,  1,  1,  8,  1,  1,  5,  1,  1, 10,  1,\n",
       "         1,  1,  5,  2,  1,  1,  3,  3,  1,  1,  1,  1,  1,  6,  1,  1,\n",
       "         1,  8,  1,  2,  1,  1,  1,  4,  3,  1,  5,  1,  1,  3,  3,  1,\n",
       "         1,  1,  1,  1,  1,  1,  2,  3,  1,  1,  1,  2,  5,  1]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "co.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5ccdff-be42-4768-8952-912d085b7157",
   "metadata": {},
   "source": [
    "### tf is a bag of words idf inversedocument freequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1efe2a86-f3fc-4cf8-bd17-c19d5a7f36e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = t.fit_transform([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "701ffc4c-8779-4670-a34b-6d712dc82f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names\n",
      "\n",
      " ['also' 'anyway' 'article' 'bad' 'better' 'big' 'body' 'boring' 'br'\n",
      " 'broken' 'bstudentsb' 'bthis' 'btw' 'caps' 'case' 'class' 'clean'\n",
      " 'cleancode' 'cleaned' 'cleaning' 'clearer' 'codethe' 'commas' 'data'\n",
      " 'dataset' 'div' 'dont' 'dots' 'emojis' 'ended' 'even' 'everywhere'\n",
      " 'extra' 'faster' 'filter' 'finally' 'finishing' 'footer' 'forget'\n",
      " 'forgot' 'found' 'full' 'fun' 'functions' 'half' 'head' 'hr' 'hrefsir'\n",
      " 'hsooo' 'html' 'idea' 'img' 'important' 'inlp' 'inserted' 'join'\n",
      " 'laughed' 'letters' 'like' 'lol' 'looked' 'looks' 'lower' 'lowercase'\n",
      " 'makes' 'many' 'marks' 'maybe' 'mess' 'messy' 'meta' 'middle' 'missing'\n",
      " 'model' 'multiple' 'nltk' 'nooo' 'normalize' 'numbers' 'oh' 'one' 'ones'\n",
      " 'others' 'please' 'pp' 'preprocessing' 'preprocessingi' 'ptextp'\n",
      " 'punctuation' 'question' 'randomly' 'readable' 'reason' 'regex' 'remove'\n",
      " 'removed' 'repeated' 'resub' 'said' 'section' 'semicolons' 'sentences'\n",
      " 'sir' 'slower' 'small' 'smarter' 'smiled' 'sometimes' 'sooo' 'spaces'\n",
      " 'spanandspan' 'split' 'stop' 'stopwords' 'student' 'students' 'stuff'\n",
      " 'sure' 'symbols' 'tags' 'talked' 'taught' 'text' 'textb' 'thats' 'three'\n",
      " 'times' 'tireda' 'title' 'trim' 'ugh' 'uppercase' 'used' 'without'\n",
      " 'words' 'works' 'worse' 'wow' 'wrote' 'yeah' 'yes' 'yesterdayh']\n",
      "Values\n",
      "\n",
      " [[0.0961262  0.06408413 0.06408413 0.0961262  0.16021033 0.06408413\n",
      "  0.03204207 0.0961262  0.03204207 0.03204207 0.03204207 0.03204207\n",
      "  0.03204207 0.03204207 0.03204207 0.03204207 0.12816827 0.03204207\n",
      "  0.06408413 0.03204207 0.03204207 0.03204207 0.03204207 0.0961262\n",
      "  0.03204207 0.0961262  0.03204207 0.0961262  0.06408413 0.06408413\n",
      "  0.03204207 0.0961262  0.06408413 0.03204207 0.03204207 0.03204207\n",
      "  0.03204207 0.06408413 0.03204207 0.03204207 0.0961262  0.03204207\n",
      "  0.0961262  0.03204207 0.03204207 0.03204207 0.06408413 0.03204207\n",
      "  0.03204207 0.1922524  0.03204207 0.03204207 0.03204207 0.03204207\n",
      "  0.03204207 0.0961262  0.03204207 0.0961262  0.2883786  0.0961262\n",
      "  0.03204207 0.03204207 0.06408413 0.06408413 0.0961262  0.06408413\n",
      "  0.03204207 0.03204207 0.22429447 0.03204207 0.03204207 0.03204207\n",
      "  0.03204207 0.03204207 0.03204207 0.03204207 0.03204207 0.12816827\n",
      "  0.12816827 0.12816827 0.06408413 0.03204207 0.06408413 0.0961262\n",
      "  0.03204207 0.03204207 0.03204207 0.03204207 0.25633653 0.03204207\n",
      "  0.03204207 0.16021033 0.03204207 0.03204207 0.32042067 0.03204207\n",
      "  0.03204207 0.03204207 0.16021033 0.06408413 0.03204207 0.03204207\n",
      "  0.0961262  0.0961262  0.03204207 0.03204207 0.03204207 0.03204207\n",
      "  0.03204207 0.1922524  0.03204207 0.03204207 0.03204207 0.25633653\n",
      "  0.03204207 0.06408413 0.03204207 0.03204207 0.03204207 0.12816827\n",
      "  0.0961262  0.03204207 0.16021033 0.03204207 0.03204207 0.0961262\n",
      "  0.0961262  0.03204207 0.03204207 0.03204207 0.03204207 0.03204207\n",
      "  0.03204207 0.03204207 0.06408413 0.0961262  0.03204207 0.03204207\n",
      "  0.03204207 0.06408413 0.16021033 0.03204207]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Feature names\\n\\n\",t.get_feature_names_out())\n",
    "print(\"Values\\n\\n\",tf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d60924-420c-4773-a09c-157be71a24ce",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a2f1065-fd83-44c0-b5d1-4be1ef616080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hsooo yesterdayh taught bstudentsb inlp preprocessingi yeah yeah like lower case stop words punctuation oh wow mess mess students maybe sure anyway talked talked talked commas dots dots dots question marks semicolons stuff div div said like sir remove punctuation numbers html tags pp spaces spaces spaces said makes codethe data clean clean cleancode oh yes yes yes also found sooo many stopwords like spanandspan oh nooo ugh repeated words everywhere everywhere everywhere sometimes text big letters small letters missing letters punctuation punctuation punctuation lowercase important uppercase looks bad bad bad found multiple spaces like one even worse ones sentences ended without reason others ended randomly inserted middle text students like hrefsir tireda said clean data remove html tags img body head remove punctuation remove stopwords remove numbers remove emojis remove extra spaces one student wrote remove stopwords please please please caps forgot half text lol lol lol used nltk regex resub split join join join functions oh many symbols btw html html also found lowercase makes ptextp readable readable readable readable readable like like article article finishing section class section laughed smiled cleaned cleaned data removed numbers stopwords stopwords stopwords stopwords extra spaces finally dataset looked better better better better big mess mess mess mess mess full tags div meta h h title broken html br hr hr punctuation text like bthis messy textb others like idea anyway cleaning text boring boring boring also fun fun fun makes model smarter faster better clearer slower slower slower said dont forget remove punctuation stopwords numbers emojis html tags said yes sir yes sir three times three times three times footer footer thats preprocessing works works works lower clean trim remove filter normalize normalize normalize normalize'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_text = \"\".join([port.stem(word) for word in text])\n",
    "stem_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c521519-ce5a-4bdf-a903-22af4750b7a3",
   "metadata": {},
   "source": [
    "### Lematization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b06e669-b122-40a8-970c-0b81d5b59792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hsooo yesterdayh taught bstudentsb inlp preprocessingi yeah yeah like lower case stop words punctuation oh wow mess mess students maybe sure anyway talked talked talked commas dots dots dots question marks semicolons stuff div div said like sir remove punctuation numbers html tags pp spaces spaces spaces said makes codethe data clean clean cleancode oh yes yes yes also found sooo many stopwords like spanandspan oh nooo ugh repeated words everywhere everywhere everywhere sometimes text big letters small letters missing letters punctuation punctuation punctuation lowercase important uppercase looks bad bad bad found multiple spaces like one even worse ones sentences ended without reason others ended randomly inserted middle text students like hrefsir tireda said clean data remove html tags img body head remove punctuation remove stopwords remove numbers remove emojis remove extra spaces one student wrote remove stopwords please please please caps forgot half text lol lol lol used nltk regex resub split join join join functions oh many symbols btw html html also found lowercase makes ptextp readable readable readable readable readable like like article article finishing section class section laughed smiled cleaned cleaned data removed numbers stopwords stopwords stopwords stopwords extra spaces finally dataset looked better better better better big mess mess mess mess mess full tags div meta h h title broken html br hr hr punctuation text like bthis messy textb others like idea anyway cleaning text boring boring boring also fun fun fun makes model smarter faster better clearer slower slower slower said dont forget remove punctuation stopwords numbers emojis html tags said yes sir yes sir three times three times three times footer footer thats preprocessing works works works lower clean trim remove filter normalize normalize normalize normalize'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_text = \"\".join([lm.lemmatize(word) for word in text])\n",
    "lm_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8a32ed-27c0-45cd-ab3a-24bf6b962e61",
   "metadata": {},
   "source": [
    "### Word of Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5bf9b56-be54-433b-b087-8cd170184e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok.fit_on_texts([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb0c3f92-af49-4ff5-b38b-38d04b77bea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'remove': 1,\n",
       " 'like': 2,\n",
       " 'punctuation': 3,\n",
       " 'stopwords': 4,\n",
       " 'mess': 5,\n",
       " 'html': 6,\n",
       " 'spaces': 7,\n",
       " 'said': 8,\n",
       " 'yes': 9,\n",
       " 'text': 10,\n",
       " 'readable': 11,\n",
       " 'better': 12,\n",
       " 'oh': 13,\n",
       " 'numbers': 14,\n",
       " 'tags': 15,\n",
       " 'clean': 16,\n",
       " 'normalize': 17,\n",
       " 'talked': 18,\n",
       " 'dots': 19,\n",
       " 'div': 20,\n",
       " 'sir': 21,\n",
       " 'makes': 22,\n",
       " 'data': 23,\n",
       " 'also': 24,\n",
       " 'found': 25,\n",
       " 'everywhere': 26,\n",
       " 'letters': 27,\n",
       " 'bad': 28,\n",
       " 'please': 29,\n",
       " 'lol': 30,\n",
       " 'join': 31,\n",
       " 'boring': 32,\n",
       " 'fun': 33,\n",
       " 'slower': 34,\n",
       " 'three': 35,\n",
       " 'times': 36,\n",
       " 'works': 37,\n",
       " 'yeah': 38,\n",
       " 'lower': 39,\n",
       " 'words': 40,\n",
       " 'students': 41,\n",
       " 'anyway': 42,\n",
       " 'many': 43,\n",
       " 'big': 44,\n",
       " 'lowercase': 45,\n",
       " 'one': 46,\n",
       " 'ended': 47,\n",
       " 'others': 48,\n",
       " 'emojis': 49,\n",
       " 'extra': 50,\n",
       " 'article': 51,\n",
       " 'section': 52,\n",
       " 'cleaned': 53,\n",
       " 'h': 54,\n",
       " 'hr': 55,\n",
       " 'footer': 56,\n",
       " 'hsooo': 57,\n",
       " 'yesterdayh': 58,\n",
       " 'taught': 59,\n",
       " 'bstudentsb': 60,\n",
       " 'inlp': 61,\n",
       " 'preprocessingi': 62,\n",
       " 'case': 63,\n",
       " 'stop': 64,\n",
       " 'wow': 65,\n",
       " 'maybe': 66,\n",
       " 'sure': 67,\n",
       " 'commas': 68,\n",
       " 'question': 69,\n",
       " 'marks': 70,\n",
       " 'semicolons': 71,\n",
       " 'stuff': 72,\n",
       " 'pp': 73,\n",
       " 'codethe': 74,\n",
       " 'cleancode': 75,\n",
       " 'sooo': 76,\n",
       " 'spanandspan': 77,\n",
       " 'nooo': 78,\n",
       " 'ugh': 79,\n",
       " 'repeated': 80,\n",
       " 'sometimes': 81,\n",
       " 'small': 82,\n",
       " 'missing': 83,\n",
       " 'important': 84,\n",
       " 'uppercase': 85,\n",
       " 'looks': 86,\n",
       " 'multiple': 87,\n",
       " 'even': 88,\n",
       " 'worse': 89,\n",
       " 'ones': 90,\n",
       " 'sentences': 91,\n",
       " 'without': 92,\n",
       " 'reason': 93,\n",
       " 'randomly': 94,\n",
       " 'inserted': 95,\n",
       " 'middle': 96,\n",
       " 'hrefsir': 97,\n",
       " 'tireda': 98,\n",
       " 'img': 99,\n",
       " 'body': 100,\n",
       " 'head': 101,\n",
       " 'student': 102,\n",
       " 'wrote': 103,\n",
       " 'caps': 104,\n",
       " 'forgot': 105,\n",
       " 'half': 106,\n",
       " 'used': 107,\n",
       " 'nltk': 108,\n",
       " 'regex': 109,\n",
       " 'resub': 110,\n",
       " 'split': 111,\n",
       " 'functions': 112,\n",
       " 'symbols': 113,\n",
       " 'btw': 114,\n",
       " 'ptextp': 115,\n",
       " 'finishing': 116,\n",
       " 'class': 117,\n",
       " 'laughed': 118,\n",
       " 'smiled': 119,\n",
       " 'removed': 120,\n",
       " 'finally': 121,\n",
       " 'dataset': 122,\n",
       " 'looked': 123,\n",
       " 'full': 124,\n",
       " 'meta': 125,\n",
       " 'title': 126,\n",
       " 'broken': 127,\n",
       " 'br': 128,\n",
       " 'bthis': 129,\n",
       " 'messy': 130,\n",
       " 'textb': 131,\n",
       " 'idea': 132,\n",
       " 'cleaning': 133,\n",
       " 'model': 134,\n",
       " 'smarter': 135,\n",
       " 'faster': 136,\n",
       " 'clearer': 137,\n",
       " 'dont': 138,\n",
       " 'forget': 139,\n",
       " 'thats': 140,\n",
       " 'preprocessing': 141,\n",
       " 'trim': 142,\n",
       " 'filter': 143}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae742f4-2281-4295-b660-a831bab8cdeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
